{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, train, evaluate MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch provides two high-level features:\n",
    "* Tensor computing (like NumPy) with strong acceleration via graphics processing units (GPU)\n",
    "* Deep neural networks biult on an automatic differentiation system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuarcy\n",
    "def AccuarcyCompute(pred, label):\n",
    "    pred = pred.cpu().data.numpy()\n",
    "    label = label.cpu().data.numpy()\n",
    "    test_np = (np.argmax(pred,1) == label)\n",
    "    test_np = np.float32(test_np)\n",
    "    return np.mean(test_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST dataset\n",
    "#### Handwritten Digit Recognition\n",
    "In this tutorial, we'll give you a step by step walk-through of how to build a hand-written digit classifier using the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset. For someone new to deep learning, this exercise is arguably the \"Hello World\" equivalent.\n",
    "\n",
    "MNIST is a widely used dataset for the hand-written digit classification task. It consists of 70,000 labeled 28x28 pixel grayscale images of hand-written digits. The dataset is split into 60,000 training images and 10,000 test images. There are 10 classes (one for each of the 10 digits). The task at hand is to train a model using the 60,000 training images and subsequently test its classification accuracy on the 10,000 test images.  \n",
    "\n",
    "![png](https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/example/mnist.png) \n",
    "<!---![png](mnist.png)-->\n",
    "\n",
    "**Figure 1:** Sample images from the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build MLP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/mlp_mnist.png)\n",
    "<!-- ![png](mlp_mnist.png) -->\n",
    "\n",
    "**Figure 2:** MLP network architecture for MNIST. (Last layer is Dense layer without activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last fully connected layer often has its hidden size equal to the number of output classes in the dataset. While we could use the Softmax activation to map values to a probability score for each class of output, we will use special loss for this puporse uring the training stage( which process layer through softmax function and then loss function computes the [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) between the probability distribution (softmax output) predicted by the network and the true probability distribution given by the label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have defined the following MLP components: \n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=50, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def build_mlp():\n",
    "    mlp = nn.Sequential(\n",
    "        nn.Linear(784, 50),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(50, 10)\n",
    "    )\n",
    "    print(f'You have defined the following MLP components: \\n{mlp}')\n",
    "    return mlp\n",
    "\n",
    "mlp = build_mlp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In pytorch, load minist is very simple, just use torchvision.datasets.MNIST(), torch.utils.data.DataLoader()\n",
    "\n",
    " - torchvision.datasets.MNIST(): download and import dataset\n",
    " - torch.utils.data.DataLoader(): transfer dataset to be iterable, so that for loop can handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368cd4c86b5c4da2a243b9f889400e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST\\raw\\train-images-idx3-ubyte.gz to data/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf5c94c550f497f87732d907abf70f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST\\raw\\train-labels-idx1-ubyte.gz to data/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec304a6c7314da5ba618a4940925df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to data/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b171ffc7834bbda3269d7a1e879cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data/MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jamesk\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:469: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset\n",
    "train_set = datasets.MNIST(\"data/\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_set = datasets.MNIST(\"data/\", train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "# batch_size means number of samples (data) to be handled by program in one iteration\n",
    "train_dataset = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "test_dataset = torch.utils.data.DataLoader(test_set, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image batches are commonly represented by a 4-D array with shape `(batch_size, num_channels, width, height)`. For the MNIST dataset, since the images are grayscale, there is only one color channel. Also, the images are 28x28 pixels, and so each image has width and height equal to 28. Therefore, the shape of input is `(batch_size, 1, 28, 28)`. Another important consideration is the order of input samples. When feeding training examples, it is critical that we don't feed samples with the same label in succession. Doing so can slow down training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function and optimizer\n",
    "\n",
    "To optimize the model, we need to define a objective between the prediction and target\n",
    "\n",
    "A simple method is to use square loss. Nevertheless, this approach does not consider the meaning of model outputs. \n",
    "\n",
    "Instead, for classification task, we often use another loss called **cross-entropy (CE) loss**. \n",
    "\n",
    "Suppose the target is a one-hot vector $\\vec{t}$ with $t_k=1$, and the model prediction is denoted as $\\vec{v}$ as above, then the cross-entropy loss is defined as:\n",
    "$$L_{\\text{CE}}(\\vec{v}, \\vec{t}) = -\\sum_{j=1}^C t_j \\log(v_j) = -\\log(v_k)$$\n",
    "Since all elements of $\\vec{v}$ are between 0 and 1, we must always have $\\log(v_j)<0$, therefore the objective is always non-negative. \n",
    "\n",
    "And we have $L_{\\text{CE}}(\\vec{v}, \\vec{t}) = 0$ if and only if $v_k=1$ (and $\\vec{v}=\\vec{t}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mlp.parameters(): all parameters of the defined mlp.\n",
    "You should tune 'lr' (not too big and not too small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer, optimizers are in torch.optim pkg\n",
    "optimizer = torch.optim.SGD(mlp.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# define loss_func, loss_func are in torch.nn pkg\n",
    "lossfunc = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Typically, one runs the training until convergence, which means that we have learned a good set of model parameters (weights + biases) from the train data. For the purpose of this tutorial, we’ll run training for 3 epochs and stop. An epoch is one full pass over the entire train data.\n",
    "\n",
    "We will take following steps for training:\n",
    "\n",
    "- Define Accuracy evaluation metric over training data.\n",
    "- Loop over inputs for every epoch.\n",
    "- Forward input through network to get output.\n",
    "- Compute loss with output and label inside record scope.\n",
    "- Backprop gradient inside record scope.\n",
    "- Update evaluation metric and parameters with gradient descent.\n",
    "\n",
    "Loss function takes (output, label) pairs and computes a scalar loss for each sample in the mini-batch. The scalars measure how far each output is from the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch, training accuracy  0.93\n",
      "1 epoch, training accuracy  0.95\n",
      "2 epoch, training accuracy  0.96\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for i_epoch in range(epochs):\n",
    "    \n",
    "    for idx, (inputs, labels) in enumerate(train_dataset):\n",
    "        \n",
    "        # convert from shape (batch_size, 1, 28, 28) to shape (batch_size, 784)\n",
    "        inputs = inputs.view(-1, 28*28)\n",
    "        \n",
    "        # zero the paraeter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # make prediction\n",
    "        outputs = mlp(inputs)\n",
    "        # compute the loss\n",
    "        loss = lossfunc(outputs, labels)\n",
    "        # backpropagation to compute the gradients\n",
    "        loss.backward()\n",
    "        # update the model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(i_epoch, \"epoch, training accuracy \", AccuarcyCompute(outputs,labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test our model, prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the above training completes, we can evaluate the trained model by running predictions on testing dataset. Since the dataset also has labels for all test images, we can compute the accuracy metric over validation data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy: 0.9402000027894973\n"
     ]
    }
   ],
   "source": [
    "acc_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, labels) in enumerate(test_dataset):\n",
    "        \n",
    "        # again, reshape input\n",
    "        inputs = inputs.view(-1, 28*28)\n",
    "        \n",
    "        # make prediction\n",
    "        outputs = mlp(inputs)\n",
    "        \n",
    "        # compute and store accuracy score\n",
    "        acc_list.append(AccuarcyCompute(outputs, labels))\n",
    "\n",
    "print(\"Testing accuracy:\", sum(acc_list) / len(acc_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-differential, computational graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable & autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad tensor(2.)\n",
      "w.grad tensor(3.)\n",
      "b.grad tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Create tensors\n",
    "x = Variable(torch.tensor(3.), requires_grad=True)\n",
    "w = Variable(torch.tensor(2.), requires_grad=True)\n",
    "b=  Variable(torch.tensor(3.), requires_grad=True)\n",
    "\n",
    "# Build a computational graph\n",
    "y = w * x + b\n",
    "\n",
    "# Compute gradients\n",
    "y.backward()\n",
    "\n",
    "# Print out the gradients\n",
    "print(\"x.grad\", x.grad)\n",
    "print(\"w.grad\", w.grad)\n",
    "print(\"b.grad\", b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad: tensor(0.1966)\n",
      "torch.sigmoid: tensor(0.1966, grad_fn=<MulBackward0>)\n",
      "manually compute: tensor(0.1966, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Create tensors\n",
    "x = Variable(torch.tensor(-1.), requires_grad=True)\n",
    "y = 1. / (1 + torch.exp(-x))\n",
    "\n",
    "# Compute gradients\n",
    "y.backward()\n",
    "\n",
    "# Print out the gradients\n",
    "print(\"x.grad:\", x.grad)\n",
    "print(\"torch.sigmoid:\", torch.sigmoid(x)*(1-torch.sigmoid(x)))\n",
    "print(\"manually compute:\", 1./(1+torch.exp(-x)) * (1-1./(1+torch.exp(-x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cd64003d8352f8d7b993f15427237127727ba7aec5d266ff4f84423c7bb16212"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
